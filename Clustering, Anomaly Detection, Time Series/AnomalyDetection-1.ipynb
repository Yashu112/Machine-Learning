{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff92e0d-cc0d-48cb-8827-535f34f1d0a8",
   "metadata": {},
   "source": [
    "# Anomaly Detection-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11de760-b361-4666-ad5d-a0129ff9b9da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5146b454-0620-4334-b4da-e2db839443da",
   "metadata": {},
   "source": [
    "Q1. What is anomaly detection and what is its purpose?\n",
    "\n",
    "Ans. Anomaly detection is a technique used in various fields, including data science, machine learning, and cybersecurity, to identify patterns or instances that deviate significantly from the norm or expected behavior within a dataset or system. The primary purpose of anomaly detection is to detect unusual or rare events or observations that may be indicative of errors, fraud, defects, or other noteworthy deviations from the typical behavior. It plays a crucial role in various applications, including:\n",
    "\n",
    "1. **Fraud Detection**: Anomaly detection is widely used in financial institutions to identify fraudulent transactions. Unusual patterns of spending or transactions can be flagged as anomalies, helping to prevent unauthorized or fraudulent activities.\n",
    "\n",
    "2. **Network Security**: In cybersecurity, anomaly detection is used to monitor network traffic and identify unusual or suspicious behavior, such as a sudden increase in data transfer, unauthorized access attempts, or malware activity.\n",
    "\n",
    "3. **Industrial Quality Control**: In manufacturing and industrial settings, anomaly detection is employed to monitor the production process for defects or abnormalities in products. Deviations from standard quality metrics can trigger alerts or automatic adjustments.\n",
    "\n",
    "4. **Healthcare**: Anomaly detection can be used in healthcare to identify unusual patient health metrics, such as abnormal vital signs or irregular medical test results, which could indicate underlying health issues.\n",
    "\n",
    "5. **Predictive Maintenance**: In machinery and equipment maintenance, anomaly detection can help predict when a machine is likely to fail based on deviations from normal operational behavior, allowing for proactive maintenance to prevent costly breakdowns.\n",
    "\n",
    "6. **Environmental Monitoring**: Anomaly detection is used to identify unusual patterns in environmental data, such as air quality measurements or weather data, which could signal pollution events or extreme weather conditions.\n",
    "\n",
    "7. **Retail**: Retailers can use anomaly detection to identify unusual buying patterns, such as sudden spikes or drops in sales, which may require investigation or adjustments to inventory and pricing strategies.\n",
    "\n",
    "8. **Energy Management**: In energy systems, anomaly detection helps identify unusual patterns in energy consumption, potentially indicating equipment malfunctions, energy theft, or inefficiencies that need to be addressed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e7e640-4c57-4a1b-907c-dcac1f2df346",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3fe40ec-63a8-43e9-a303-30a7483d7716",
   "metadata": {},
   "source": [
    "Q2. What are the key challenges in anomaly detection?\n",
    "\n",
    "Ans. Anomaly detection is a valuable technique, but it comes with several key challenges, many of which are inherent to the complexity and diversity of real-world data and systems. Some of the primary challenges in anomaly detection include:\n",
    "\n",
    "1. **Unlabeled Data**: In many real-world scenarios, labeled data (instances that are explicitly marked as normal or anomalous) is scarce or unavailable. This makes it challenging to train supervised anomaly detection models and often requires the use of unsupervised or semi-supervised techniques.\n",
    "\n",
    "2. **Imbalanced Data**: Anomalies are typically rare events compared to normal instances. This class imbalance can lead to models that have a bias toward normal data and may struggle to detect anomalies effectively.\n",
    "\n",
    "3. **Data Quality**: Noisy or incomplete data can make it difficult to distinguish genuine anomalies from data artifacts or errors. Data preprocessing and cleaning are crucial but can be challenging tasks in themselves.\n",
    "\n",
    "4. **Concept Drift**: Real-world data is often dynamic, and the concept of what is considered normal behavior may change over time. Anomaly detection models need to adapt to these changes to remain effective.\n",
    "\n",
    "5. **Scalability**: Some anomaly detection algorithms can be computationally expensive, especially when dealing with large datasets. Scalability can be a significant challenge, particularly in real-time or high-frequency monitoring applications.\n",
    "\n",
    "6. **Interpretability**: While some machine learning models provide excellent anomaly detection performance, they may lack interpretability. Understanding why a particular instance is classified as an anomaly can be crucial for decision-making in many applications.\n",
    "\n",
    "7. **False Positives**: Anomaly detection models may generate false positives, flagging normal instances as anomalies. Reducing the false positive rate while maintaining a high true positive rate is a critical trade-off.\n",
    "\n",
    "8. **Anomaly Definition**: Defining what constitutes an anomaly can be subjective and context-dependent. Different stakeholders may have varying definitions of what is abnormal behavior.\n",
    "\n",
    "9. **Rare Anomalies vs. Common Anomalies**: Anomaly detection algorithms may perform well at detecting rare, extreme anomalies but struggle with anomalies that are more subtle or occur frequently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c0c9f5-191e-4bd2-8301-b7d4f108d847",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "607e7425-91a4-44b3-b6d4-023607efc3ee",
   "metadata": {},
   "source": [
    "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "\n",
    "Ans. Unsupervised anomaly detection and supervised anomaly detection are two distinct approaches used to identify anomalies in data, and they differ primarily in their use of labeled or unlabeled data during the training process:\n",
    "\n",
    "1. **Data Requirements**: Unsupervised methods work with unlabeled data, whereas supervised methods require labeled data.\n",
    "\n",
    "2.  **Algorithm Behavior**: Unsupervised methods focus on learning the underlying structure or patterns in the data without any guidance. They aim to find instances that deviate significantly from the typical behavior observed in the training data.  Supervised methods learn from the labeled examples and aim to build a model that can distinguish between normal and anomalous instances based on the provided labels\n",
    "\n",
    "3. **Techniques**: Common unsupervised anomaly detection techniques include clustering-based methods (e.g., DBSCAN) and density-based methods (e.g., isolation forests, local outlier factor). These algorithms identify anomalies as data points that are isolated from dense clusters. ommon supervised techniques for anomaly detection include various classification algorithms like decision trees, support vector machines, logistic regression, and neural networks. These models are trained to predict the class labels (normal or anomalous) of new data points.\n",
    "\n",
    "2. **Training Process**: Unsupervised methods learn patterns and structures from the data itself, while supervised methods learn from both the data and the labeled information.\n",
    "\n",
    "3. **Flexibility**: Unsupervised methods are more flexible and can adapt to various types of anomalies without the need for predefined labels. Supervised methods require labeled examples, which can be limiting if the anomalies change or are unknown.\n",
    "\n",
    "4. **Scalability**: Unsupervised methods are often preferred when dealing with large datasets and evolving anomalies because labeling large datasets can be time-consuming and costly.\n",
    "\n",
    "5. **Interpretability**: Supervised methods may provide more interpretable results since they are explicitly trained to classify data points as normal or anomalous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3a5421-cf4c-4391-bc03-8f066ecadebe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d1e780e-a65c-4c61-971d-c386577c3c15",
   "metadata": {},
   "source": [
    "Q4. What are the main categories of anomaly detection algorithms?\n",
    "\n",
    "Ans. Anomaly detection algorithms can be categorized into several main categories, each of which utilizes different techniques and approaches to identify anomalies in data. The main categories of anomaly detection algorithms include:\n",
    "\n",
    "1. **Statistical Methods**:\n",
    "   - **Z-Score/Standard Score**: This method calculates the z-score for each data point and flags those with z-scores exceeding a certain threshold as anomalies.\n",
    "   - **Percentile-based**: These methods use percentiles (e.g., 95th percentile) to identify values that fall beyond a specified percentile as anomalies.\n",
    "\n",
    "2. **Distance-Based Methods**:\n",
    "   - **K-Nearest Neighbors (KNN)**: KNN measures the distance between data points and their k-nearest neighbors. Data points with neighbors at a greater distance are considered anomalies.\n",
    "   - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: DBSCAN identifies anomalies as data points that do not belong to any dense cluster.\n",
    "\n",
    "3. **Density-Based Methods**:\n",
    "   - **Isolation Forest**: Isolation Forests create random decision trees and isolate anomalies as data points that require fewer splits to be isolated.\n",
    "   - **Local Outlier Factor (LOF)**: LOF measures the local density of data points and identifies those with significantly lower density as anomalies.\n",
    "\n",
    "4. **Clustering-Based Methods**:\n",
    "   - **K-Means Clustering**: Anomalies are data points that do not cluster well with others or are far from cluster centroids.\n",
    "   - **Hierarchical Clustering**: Anomalies are detected by analyzing the hierarchical structure of clusters.\n",
    "\n",
    "5. **SVM-Based Methods**: One-Class SVM: One-Class Support Vector Machines create a boundary around the normal data, classifying data points outside the boundary as anomalies.\n",
    "\n",
    "6. **Ensemble Methods**:\n",
    "   - **Combining Multiple Algorithms**: Ensembles combine the results of multiple anomaly detection algorithms to improve overall accuracy and robustness.\n",
    "\n",
    "10. **Unsupervised and Semi-Supervised Methods**:\n",
    "    - **Unsupervised Learning**: Algorithms in this category do not require labeled data and aim to identify anomalies solely based on the data distribution.\n",
    "    - **Semi-Supervised Learning**: These methods leverage a small amount of labeled data to enhance anomaly detection in an unsupervised setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aefc98-0806-42e3-85d9-7fbb688cc388",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36d8df8f-bf0d-45d9-bb0e-4532ad80e05b",
   "metadata": {},
   "source": [
    "Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "\n",
    "Ans. Distance-based anomaly detection methods rely on specific assumptions and characteristics of data to identify anomalies. The main assumptions made by distance-based anomaly detection methods include:\n",
    "\n",
    "1. **Normal Data Is Dense**: Distance-based methods assume that normal data points tend to cluster together and form dense regions in the feature space. In contrast, anomalies are expected to be sparse and isolated from these dense clusters.\n",
    "\n",
    "2. **Distance Metric Measures Dissimilarity**: These methods use a distance metric (e.g., Euclidean distance, Mahalanobis distance) to quantify the dissimilarity between data points. The assumption is that similar data points will have a smaller distance between them, while dissimilar or anomalous data points will have a larger distance.\n",
    "\n",
    "3. **Anomalies Are Far from Neighbors**: Distance-based algorithms often use the concept of nearest neighbors to identify anomalies. The assumption is that anomalies have few or no close neighbors in the dataset. If a data point has the majority of its nearest neighbors at a considerable distance, it is likely considered an anomaly.\n",
    "\n",
    "4. **Threshold-Based Detection**: Many distance-based methods use a predefined threshold or percentile value to distinguish between normal and anomalous data points. Data points with distances exceeding this threshold are classified as anomalies.\n",
    "\n",
    "5. **Assumption of Homogeneity**: Some distance-based methods assume that the data distribution is homogeneous, meaning that the density of data points is relatively consistent across the feature space. In reality, this assumption may not always hold.\n",
    "\n",
    "6. **Noisy Data Handling**: Distance-based methods are sensitive to noisy data points because outliers or noise can significantly impact distance calculations. Preprocessing and noise handling techniques are often required to ensure robustness.\n",
    "\n",
    "7. **Euclidean Distance**: Many distance-based methods rely on the Euclidean distance metric, which assumes that the data features are continuous and have a meaningful notion of distance. When dealing with categorical data or data with mixed types, distance calculations may need to be adapted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34037207-f7b7-48a2-af3b-b20400540d08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34748f35-85fc-4120-adb8-15ee88d40696",
   "metadata": {},
   "source": [
    "Q6. How does the LOF algorithm compute anomaly scores?\n",
    "\n",
    "Ans. The Local Outlier Factor (LOF) algorithm is a popular method for anomaly detection, particularly in high-dimensional data. LOF computes anomaly scores for each data point in a dataset to identify anomalies. The key idea behind LOF is to measure the local density of data points relative to their neighbors. Here's how the LOF algorithm computes anomaly scores:\n",
    "\n",
    "1. **Compute k-Nearest Neighbors (k-NN)**:\n",
    "   - For each data point in the dataset, LOF starts by finding its k nearest neighbors based on a chosen distance metric (e.g., Euclidean distance).\n",
    "\n",
    "2. **Compute Reachability Distance**:\n",
    "   - For each data point, calculate the reachability distance of that point with respect to its k nearest neighbors. The reachability distance of point A with respect to point B is the maximum of the distance between A and B and the kth nearest neighbor distance of point B. Mathematically, it can be defined as:\n",
    "   \n",
    "     Reachability_Distance(A, B) = max(Distance(A, B), kth_NearestNeighbor_Distance(B))\n",
    "\n",
    "   - This step quantifies how \"reachable\" a data point is from its neighbors. If a point is close to its neighbors, its reachability distance will be relatively small; if it's far from its neighbors, the reachability distance will be larger.\n",
    "\n",
    "3. **Calculate Local Reachability Density (LRD)**:\n",
    "   - For each data point, calculate the local reachability density (LRD). LRD for a data point A is the inverse of the average reachability distance of A with respect to its k nearest neighbors. It is calculated as:\n",
    "   \n",
    "     LRD(A) = 1 / (Average_Reachability_Distance(A, Neighbors(A)))\n",
    "\n",
    "   - The LRD provides an estimate of the local density of data points around A. High LRD values indicate that A is in a dense region, while low LRD values suggest that A is in a sparse region.\n",
    "\n",
    "4. **Calculate Local Outlier Factor (LOF)**:\n",
    "   - Finally, the LOF score for each data point is computed by comparing its LRD to the LRDs of its k nearest neighbors. It is defined as the ratio of the average LRD of the k nearest neighbors of the data point to the LRD of the data point itself:\n",
    "\n",
    "     LOF(A) = (Average_LRD(Neighbors(A))) / LRD(A)\n",
    "\n",
    "   - If the LOF score is close to 1, it suggests that the data point has a similar local density as its neighbors and is not an outlier. However, if the LOF score is significantly greater than 1, it indicates that the data point is less dense than its neighbors and is considered an outlier or anomaly.\n",
    "\n",
    "5. **Anomaly Threshold**:\n",
    "   - The LOF values can be used to rank data points based on their anomaly scores. A higher LOF value suggests a higher likelihood of being an anomaly. You can set a threshold to determine which data points are considered anomalies based on their LOF scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d7600-c4a5-41ef-afef-b7467711151f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9cccfc9-d7b3-4998-9212-b48746f5168d",
   "metadata": {},
   "source": [
    "Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "\n",
    "Ans. The Isolation Forest algorithm is an ensemble-based method for anomaly detection that is particularly effective for high-dimensional datasets. It uses a collection of decision trees to isolate anomalies by exploiting the fact that anomalies are typically rare and can be identified by their tendency to have shorter paths in a tree-based partitioning of the data. The key parameters of the Isolation Forest algorithm include:\n",
    "\n",
    "1. **n_estimators**: This parameter determines the number of isolation trees to build in the ensemble. A larger number of trees generally leads to better performance in detecting anomalies, but it also increases computation time.\n",
    "\n",
    "2. **max_samples**: It specifies the maximum number of data points sampled to create each isolation tree. A smaller value results in each tree having fewer data points and may make the algorithm faster but potentially less accurate. A common value is often set to \"auto,\" which selects a default subsample size.\n",
    "\n",
    "3. **max_features**: This parameter controls the maximum number of features (or dimensions) to consider when splitting a node in each isolation tree. It helps reduce the potential for overfitting. A common value is often set to \"auto,\" which uses the square root of the number of features.\n",
    "\n",
    "4. **contamination**: This parameter sets the expected proportion of anomalies in the dataset. It is essentially a prior estimate of the anomaly rate, which helps determine the decision threshold for identifying anomalies. The default value is typically set to 0.1 (10%) but should be adjusted based on the actual anomaly rate in the data.\n",
    "\n",
    "5. **bootstrap**: This binary parameter controls whether or not bootstrap samples are used to create the isolation trees. Setting it to \"True\" enables bootstrap sampling, which can introduce randomness into the process and may help with robustness.\n",
    "\n",
    "6. **random_state**: This parameter is used to seed the random number generator for reproducibility. It ensures that the same results are obtained when running the algorithm with the same parameters and data.\n",
    "\n",
    "7. **n_jobs**: It specifies the number of CPU cores to use when fitting the model in parallel. Setting it to -1 uses all available cores.\n",
    "\n",
    "8. **verbose**: This parameter controls the verbosity of the algorithm's output. A higher value leads to more detailed progress and diagnostic messages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba4d988-07e6-4a56-8e02-b840a31924c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bbaa960-7a6b-492d-97f5-16dc810ff1f7",
   "metadata": {},
   "source": [
    "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?\n",
    "\n",
    "Ans. In K-Nearest Neighbors (KNN) anomaly detection, the anomaly score for a data point is typically based on its relationship with its k nearest neighbors. In our scenario, we have K = 10, and data point has only 2 neighbors of the same class within a radius of 0.5.\n",
    "\n",
    "```python \n",
    "anomaly_score = 1.0 - (same_class_neighbors / k)\n",
    "```\n",
    "we have same_class_neighbors=2, k=10, so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa7a3ba4-795e-4257-8573-5bf3af49da82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "same_class_neighbors=2\n",
    "k=10\n",
    "anomaly_score = 1.0 - (same_class_neighbors / k)\n",
    "print(\"Anomaly Score:\", anomaly_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a3254c-b91b-448b-9aa5-951bf66ef22f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58fbb2bd-d4cc-4838-838a-971d2cd5a885",
   "metadata": {},
   "source": [
    "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "length of the trees?\n",
    "\n",
    "Ans. Solution is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d60ff040-b7d5-4c5a-90da-6b18d42f45c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score: 0.9975297801838289\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "num_trees = 100\n",
    "num_data_points = 3000\n",
    "average_path_length_data_point = 5.0\n",
    "\n",
    "# Calculate the expected average path length for normal data points\n",
    "expected_average_path_length_normal = 2 * np.log(num_data_points - 1) - (2 * (num_data_points - 1) / num_data_points)\n",
    "\n",
    "# Calculate the anomaly score\n",
    "anomaly_score = 2 ** (-average_path_length_data_point / (num_trees * expected_average_path_length_normal))\n",
    "\n",
    "print(\"Anomaly Score:\", anomaly_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87eefe-3f80-4940-80e4-4de0e4338759",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
