{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da8f213-7ef6-4f34-860e-af38723b5f65",
   "metadata": {},
   "source": [
    "# Regression-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e00ac6-8870-4513-a819-4d064e5d1bb9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90adef40-77a5-4476-8a7e-934541475157",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "\n",
    "Ans. R-squared, often denoted as R², is a statistical measure used to evaluate the goodness of fit of a linear regression model. It provides insights into how well the independent variable(s) in a linear regression model explain the variation in the dependent variable.\n",
    "\n",
    "The formula for $R^2$ is as shown:\n",
    "$$ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} $$\n",
    "\n",
    "where,\n",
    "\n",
    "- **$SS_{tot}$** is the sum of the squared differences between each observed dependent variable value ($y$) and the mean of the dependent variable ($\\bar{y}$). Mathematically:\n",
    "$$ SS_{tot} = \\sum\\limits_{i=1}^{n}\\left( y_i - \\bar{y} \\right)^2 $$\n",
    "\n",
    "- **$SS_{res}$** is the sum of the squared differences between the observed dependent variable values ($y$) and predicted values ($\\hat{y_i}$ or $h_{\\theta}(x)$) obtained from the linear regression model. Mathematically:\n",
    "$$ SS_{res} = \\sum\\limits_{i=1}^{n}\\left( y_i - \\hat{y_i} \\right)^2 $$\n",
    "\n",
    "Thus we get:\n",
    "\n",
    "$$ R^2 = 1 - \\frac{\\sum\\limits_{i=1}^{n}\\left( y_i - \\bar{y} \\right)^2}{\\sum\\limits_{i=1}^{n}\\left( y_i - \\hat{y_i} \\right)^2 } $$\n",
    "\n",
    "\n",
    "R² ranges from 0 to 1, where 0 indicates that the model does not explain any of the variance in the dependent variable, and 1 indicates that the model explains all of the variance. In practical terms, R² is usually between 0 and 1.\n",
    "\n",
    "R-squared represents the goodness of fit of the linear regression model. A higher R² value indicates that a larger proportion of the variance in the dependent variable is explained by the independent variable(s), suggesting that the model fits the data well. Conversely, a lower R² suggests that the model does not explain much of the variance, and it might not be a good fit for the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383d994-d595-4785-9db7-87038d09853f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a439c9b-ced7-4f41-8c8a-20390dca5247",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "Ans. Adjusted R-squared is a modified version of the regular R-squared (R²) used in linear regression analysis. While both R² and adjusted R-squared serve as measures of how well a regression model fits the data, adjusted R-squared takes into account the number of independent variables in the model, addressing a limitation of the regular R-squared.\n",
    "\n",
    "Regular R-squared tends to increase as more independent variables are added to a model, even if those variables don't significantly improve the model's fit. This is because R-squared is based on the proportion of the total variation explained by the model. As we add more variables, some of the variation is naturally explained, even if it's not meaningful. Adjusted R-squared, on the other hand, adjusts for model complexity by penalizing the inclusion of unnecessary variables.\n",
    "\n",
    "**Formula:** The formula for adjusted R-squared is slightly different from the regular R-squared:\n",
    "\n",
    "$$ \\text{Adjusted } R^2 = 1 - \\frac{(1 - R^2) . (n - 1) }{ (n - k - 1)}$$\n",
    "\n",
    "Where:\n",
    "- R² is the regular R-squared value.\n",
    "- n is the number of observations (data points).\n",
    "- k is the number of independent variables in the model.\n",
    "\n",
    "Adjusted R-squared penalizes the inclusion of extraneous variables, which means that it will increase only if adding a new variable improves the model more than would be expected by random chance. A higher adjusted R-squared suggests that the independent variables in the model collectively have a stronger and more meaningful influence on the dependent variable, while a lower adjusted R-squared indicates that the model may be overfitting or including unnecessary variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78194226-632e-49d6-8c41-fc6b1c7fc632",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "596135fb-c5da-4b8b-9aee-1d1a9eebff91",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "Ans. Adjusted R-squared is more appropriate to use in the following situations:\n",
    "\n",
    "1. **Comparing Models:** When we are comparing multiple regression models with different numbers of independent variables, adjusted R-squared helps us determine which model provides a better balance between goodness of fit and model complexity. It penalizes the inclusion of unnecessary variables, making it a useful tool for model selection.\n",
    "\n",
    "2. **Model Selection:** When we are in the process of building a regression model and considering which independent variables to include, adjusted R-squared can guide our decisions. It encourages us to choose variables that genuinely improve the model's fit and explanatory power, rather than including variables just because they lead to higher regular R-squared values.\n",
    "\n",
    "3. **Preventing Overfitting:** Overfitting occurs when a model fits the training data too closely, capturing noise and in the data rather than the underlying relationships. Adjusted R-squared helps prevent overfitting by discouraging the inclusion of too many independent variables that do not significantly contribute to the model's ability to explain the dependent variable.\n",
    "\n",
    "5. **Interpreting Model Quality:** When we need a more conservative estimate of a model's quality, adjusted R-squared provides a more realistic assessment of how well the model generalizes to new data, as it accounts for model complexity and the risk of overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d208b0-baa5-4e72-892c-ef5bb71aa7f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0a05007-625e-4191-a4fa-bdfef84812cb",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "\n",
    "Ans. RMSE, MSE, and MAE are 3 types of cost/loss function used in regression analysis to assess the performance and accuracy of regression models. They represent the amount of error, the deviation of our predicted values from actual values.\n",
    "\n",
    "- **MAE:** It stands for *Mean Absolute Error*. It is the sum of absolute differences between the actual and predicted values.\n",
    "$$ \\text{MAE} = \\frac{1}{n} \\sum\\limits_{i=1}^{n}| y_i - \\hat{y_i} | $$ \n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "- **MSE:** It stands for *Mean Squared Error*. It is the sum of square of differences between the actual and predicted values. \n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum\\limits_{i=1}^{n}\\left( y_i - \\hat{y_i} \\right)^2 $$\n",
    "\n",
    "- **RMSE:** It stands for *Root Mean Squared Error*. It is square root of the sum of square of differences between the actual and predicted values. In simple words, it is the square root of MSE.\n",
    "\n",
    "$$ \\text{RMSE} = \\sqrt{\\text{MSE}} $$\n",
    "\n",
    "$$ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum\\limits_{i=1}^{n}\\left( y_i - \\hat{y_i} \\right)^2} $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f721ed97-3b21-4843-a441-12b1fb731654",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1061f02f-4c57-498c-915f-4c7ca919e5ba",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "\n",
    "Ans. \n",
    "- **MAE:**  \n",
    "    |Advantages|Disadvantages|\n",
    "    |---|---|\n",
    "    |Robust to outliers|Convergence takes more time|\n",
    "    |It is in the same unit as predicted variable| |\n",
    "    \n",
    "- **MSE:** \n",
    "    |Advantages|Disadvantages|\n",
    "    |---|---|\n",
    "    |Equation is differentiable|Not robust to outliers|\n",
    "    |It has only one locar or global minima|It is not in the same unit as independent variable|\n",
    "\n",
    "- **RMSE:**\n",
    "    |Advantages|Disadvantages|\n",
    "    |---|---|\n",
    "    |It is in the same unit as the predicted variable| Not robust to outliers|\n",
    "    |It is differentiable|  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f59094-4f7d-48b2-ac3e-ad15451baa71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59c132f2-1fd1-4358-b6be-48558ec1f5ec",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "\n",
    "Ans. Regularization is a set of techniques used in machine learning to prevent overfitting by adding a penalty to the model's loss function based on the complexity of the model. Regularization techniques encourage the model to have smaller parameter values, effectively reducing its capacity to fit noise in the training data and promoting better generalization to new, unseen data.\n",
    "\n",
    "Suppose we have :\n",
    "$$ h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x + \\theta_{2}x + ...$$\n",
    "\n",
    "Suppose we use Mean Squared Error as our cost function:\n",
    "$$ J(\\theta) = \\frac{1}{n} \\sum\\limits_{i=1}^{n}\\left(y_i - h_{\\theta}(x_i)\\right)^2 $$\n",
    "\n",
    "- **L1 Regularization (Lasso):** L1 regularization or Lasso Regularization adds a penalty term proportional to the absolute values of the model's parameters to the cost function. This can help in feature selection and reducing model complexity.\n",
    "    \n",
    "    The regularized function is: \n",
    "    \n",
    "    $$ J(\\theta) = \\frac{1}{n} \\sum\\limits_{i=1}^{n}\\left(y_i - h_{\\theta}(x_i)\\right)^2 + \\lambda\\sum\\limits_{i=1}^{n}|\\theta_{i}| $$\n",
    "\n",
    "\n",
    "- **L2 Regularization (Ridge):** L2 regularization adds a penalty term proportional to the squared values of the model's parameters to the loss function. It encourages the model to have smaller but non-zero weight values for all features, distributing the influence more evenly across features.\n",
    "\n",
    "    The regularized function is: \n",
    "    \n",
    "    $$ J(\\theta) = \\frac{1}{n} \\sum\\limits_{i=1}^{n}\\left(y_i - h_{\\theta}(x_i)\\right)^2 + \\lambda\\sum\\limits_{i=1}^{n}(\\theta_{i}) ^2$$\n",
    "\n",
    "We can use Lasso regression when we have many features and want automatic feature selection or a simpler model. It sets some coefficients to zero, aiding in feature pruning and reducing multicollinearity. Lasso is suitable for high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e050dc85-4983-46b4-8070-f4079a30d244",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17248b62-a436-43cf-bc99-1130492ec46b",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "\n",
    "Ans. Regularized linear models, such as Ridge and Lasso regression, help prevent overfitting in machine learning by adding a penalty term to the linear regression cost function. This penalty discourages the model from fitting the training data too closely, which can lead to overfitting.\n",
    "\n",
    "Lasso Regularization adds L1 penalty:\n",
    "$$ J(\\theta) = \\frac{1}{n} \\sum\\limits_{i=1}^{n}\\left(y_i - h_{\\theta}(x_i)\\right)^2 + \\lambda\\sum\\limits_{i=1}^{n}|\\theta_{i}| $$\n",
    "\n",
    "Ridge Rgularization adds L2 penalty:\n",
    "$$ J(\\theta) = \\frac{1}{n} \\sum\\limits_{i=1}^{n}\\left(y_i - h_{\\theta}(x_i)\\right)^2 + \\lambda\\sum\\limits_{i=1}^{n}(\\theta_{i}) ^2$$\n",
    "\n",
    "Example:\n",
    "Consider a dataset with various features (e.g., age, income, and education) and a target variable (e.g., house price). In a traditional linear regression model, all features may be used to fit the data, potentially resulting in overfitting if there are too many features relative to the number of data points.\n",
    "\n",
    "- If we use Ridge regression with an appropriate value of α, it will encourage the model to keep all features in the model but with smaller coefficients. This reduces the risk of overfitting by controlling the magnitude of the coefficients.\n",
    "\n",
    "- If we use Lasso regression, it will not only shrink the coefficients but also force some of them to be exactly zero. This results in feature selection, effectively removing less relevant features from the model, which can help combat overfitting.\n",
    "\n",
    "In both cases, regularized linear models offer a way to strike a balance between fitting the training data well and preventing overfitting, which can lead to better generalization to unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f6cf7-9283-4986-90d9-85ff54586185",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c6220bc-c19f-45e1-bfda-d776d1f21776",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "\n",
    "Ans. Here are some limitations of regularized linear models:\n",
    "\n",
    "1. **Feature Selection Biases:** While Lasso regression can perform automatic feature selection by driving some coefficients to zero, this can lead to a biased model if important features are incorrectly excluded. \n",
    "\n",
    "2. **Linear Assumption:** Regularized linear models assume a linear relationship between independent and dependent variables. If the true relationship is nonlinear, these models may not capture it accurately. In such cases, more flexible non-linear models like decision trees, random forests, or neural networks might be more appropriate.\n",
    "\n",
    "3. **Sensitivity to Hyperparameters:** The performance of regularized linear models depends on the choice of hyperparameters (e.g., the regularization strength α in Ridge and Lasso). Tuning these hyperparameters can be a non-trivial task, and suboptimal choices may lead to underfitting or overfitting.\n",
    "\n",
    "4. **Loss of Interpretability:** Regularized linear models can make the model less interpretable, especially in Lasso, where some coefficients are set to zero. In situations where interpretability is crucial, a simpler linear regression model might be preferred.\n",
    "\n",
    "5. **Data Requirements:** Regularized linear models require a sufficient amount of data to perform well, especially when the number of features is large. In high-dimensional datasets with limited samples, regularized models might struggle to provide accurate estimates.\n",
    "\n",
    "6. **Multicollinearity Challenges:** While Ridge regression can help mitigate multicollinearity, it doesn't address the fundamental issue of highly correlated independent variables. In some cases, multicollinearity can still make coefficient estimates unstable.\n",
    "\n",
    "7. **Loss of Detail:** The regularization term in Ridge and Lasso adds bias to the coefficient estimates, potentially causing some loss of detail in capturing the relationships between variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b18ba3-b5ad-4282-a1e2-8d65488c62bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "650b1b46-c88d-486f-8b16-e30a6090933f",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "Ans. If we want a more robust measure of overall model accuracy that doesn't heavily emphasize outliers, we might prefer Model B with an MAE of 8. This indicates that, on average, the model's predictions are off by 8 units, with no additional penalty for larger errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f532ba-f513-464f-a338-940161650bc9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be720675-36e9-447f-9626-35e6010acb78",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
    "\n",
    "Ans. The choice between Ridge and Lasso regularization depends on the specific characteristics of our data, the objectives of our analysis, and the trade-offs associated with each regularization method. Let's discuss the properties of Ridge and Lasso and how they might influence your choice:\n",
    "\n",
    "**Ridge Regularization:** The regularization parameter (λ or α) controls the strength of the penalty. In our example, Model A uses a relatively low λ of 0.1, which means the penalty is moderate.\n",
    "\n",
    "**Lasso Regularization:** Like Ridge, the regularization parameter (λ or α) controls the strength of the penalty. In our example, Model B uses a moderate λ of 0.5, indicating a stronger penalty compared to Ridge.\n",
    "\n",
    "**Choosing the Better Performer:**\n",
    "\n",
    "To choose the better performer between Model A (Ridge) and Model B (Lasso), we should consider the following factors:\n",
    "\n",
    "1. **Feature Selection vs. Multicollinearity:** If our dataset has many features, some of which are potentially irrelevant, and we want to automatically select the most important ones, Lasso (Model B) might be a better choice due to its feature selection property.\n",
    "\n",
    "2. **Multicollinearity:** If our data has strong multicollinearity, Ridge (Model A) might be more appropriate as it can effectively handle multicollinearity without excluding any variables entirely.\n",
    "\n",
    "3. **Interpretability:** Consider the interpretability of the model. Lasso tends to produce sparser models with fewer variables, which can be more interpretable, while Ridge retains all variables in the model.\n",
    "\n",
    "4. **Domain Knowledge:** Consider domain-specific knowledge and the importance of individual variables in our decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b72d1f-ef40-4dfd-8a04-3090c469c2b9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
