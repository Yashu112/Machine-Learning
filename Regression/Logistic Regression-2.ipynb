{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f9f050-8056-4936-9713-cf80994e763b",
   "metadata": {},
   "source": [
    "# Logistic Regression-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846151f4-89b0-4f42-b7df-74163bd3abb7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6400f503-1cf8-41f7-bdea-b1c987bfd9cc",
   "metadata": {},
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n",
    "\n",
    "Ans. Grid Search Cross-Validation (GridSearchCV) is a technique used in machine learning to systematically search for the optimal combination of hyperparameters for a given model. Its primary purpose is to automate the process of hyperparameter tuning, which is the process of finding the best hyperparameters for a machine learning algorithm to achieve the highest model performance.\n",
    "\n",
    "Here's how GridSearchCV works:\n",
    "\n",
    "1. **Hyperparameter Grid Specification**: First, we define a grid of hyperparameters that we want to search over. These hyperparameters are specified as a dictionary, where keys represent the hyperparameter names, and values are lists or ranges of values to be considered. For example:\n",
    "\n",
    "   ```python\n",
    "   param_grid = {\n",
    "       'C': [0.1, 1, 10],\n",
    "       'kernel': ['linear', 'rbf', 'poly'],\n",
    "       'gamma': [0.01, 0.1, 1]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "   In this example, `C`, `kernel`, and `gamma` are hyperparameters, and GridSearchCV will search over all possible combinations of these values.\n",
    "\n",
    "2. **Cross-Validation**: GridSearchCV employs cross-validation to evaluate each combination of hyperparameters. Typically, k-fold cross-validation is used, where the dataset is divided into k subsets (folds). The model is trained on k-1 folds and tested on the remaining fold, repeating this process k times.\n",
    "\n",
    "3. **Model Training and Evaluation**: For each combination of hyperparameters, GridSearchCV fits the model using the training data and computes a performance metric (e.g., accuracy, F1-score, or mean squared error) on the validation fold. The model's average performance across all folds is used to assess its quality.\n",
    "\n",
    "4. **Hyperparameter Tuning**: GridSearchCV systematically repeats the training and evaluation process for all combinations of hyperparameters defined in the grid. It keeps track of the best-performing combination of hyperparameters based on the chosen evaluation metric. For example, if we aim to maximize accuracy, GridSearchCV identifies the combination that yields the highest accuracy score.\n",
    "\n",
    "5. **Model Selection**: Once the grid search is complete, GridSearchCV returns the hyperparameters that produced the best-performing model on the validation data during cross-validation.\n",
    "\n",
    "6. **Final Model Training**: Finally, we can retrain the model using the best hyperparameters on the entire training dataset to create the final model.\n",
    "\n",
    "Grid Search Cross-Validation simplifies the process of hyperparameter tuning by systematically exploring various combinations, eliminating the need for manual tuning, and ensuring that you select the optimal set of hyperparameters for your machine learning model. It is a powerful tool for optimizing model performance and improving its ability to generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00de40-08e8-4692-b042-03e6b5d679a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e93fcb58-58c2-44af-b2c5-c9c70e9ad7dc",
   "metadata": {},
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?\n",
    "\n",
    "Ans. Grid Search Cross-Validation (GridSearchCV) and Randomized Search Cross-Validation (RandomizedSearchCV) are both techniques used for hyperparameter tuning in machine learning, but they differ in their approaches to exploring hyperparameter space:\n",
    "\n",
    "**Grid Search Cross-Validation (GridSearchCV)** evaluates all possible combinations of hyperparameters specified in the grid. GridSearchCV provides comprehensive coverage of the hyperparameter space, considering every combination within the grid. It can be computationally expensive, especially when the grid is large or when the model is complex, as it trains and evaluates models for every combination. It is deterministic. Given the same input and data, it will always produce the same set of results.\n",
    "\n",
    "**Randomized Search Cross-Validation (RandomizedSearchCV)** explores the hyperparameter space randomly by sampling hyperparameters from specified probability distributions. It randomly selects a limited number of combinations, making it more efficient for large search spaces. RandomizedSearchCV may not cover all possible hyperparameter combinations but focuses on a representative sample. It is suitable when the search space is vast, and exhaustive search would be too time-consuming or costly. It is generally less computationally expensive than GridSearchCV because it evaluates fewer combinations. It is beneficial when computational resources are limited or when you want to perform a quick search for good hyperparameters. Multiple runs may be necessary to ensure the stability of results.\n",
    "\n",
    "**When to Choose Grid Search vs. Randomized Search**:\n",
    "\n",
    "- **Grid Search**: We can use GridSearchCV when you have a small or reasonably sized search space, and we want to ensure that you explore all possible combinations thoroughly.\n",
    "\n",
    "- **Randomized Search**: We can choose RandomizedSearchCV when the hyperparameter search space is extensive, and it's impractical to perform an exhaustive search. It's beneficial when computational resources or time are limited, allowing us to quickly identify promising hyperparameter configurations. Randomized search can also be useful as an initial exploration to narrow down the search space before conducting a more detailed grid search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cffd79-08c5-4943-b190-1715cc5c3430",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf1eae10-2d10-4140-bb46-489555ba252f",
   "metadata": {},
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "\n",
    "Ans. Data leakage, also known as information leakage or data snooping, is a critical issue in machine learning where information from the training data unintentionally or inappropriately influences model training or evaluation, leading to overly optimistic performance estimates. Data leakage can severely undermine the reliability and generalization of machine learning models. It occurs when information that should not be available to the model during training or evaluation is inadvertently used.\n",
    "\n",
    "Data leakage is problematic for several reasons:\n",
    "\n",
    "1. **Overestimated Model Performance**: Data leakage can make a model appear to perform much better than it actually does. This is because the model might inadvertently learn patterns or relationships that do not exist in the real-world data but are present in the training data due to the leakage.\n",
    "\n",
    "2. **Loss of Generalization**: Models trained on data with leakage may not generalize well to new, unseen data because they have learned to rely on the leaked information, which may not be available in real-world scenarios.\n",
    "\n",
    "\n",
    "Here's an example :\n",
    "\n",
    "In a medical study aiming to predict disease risk, data leakage could occur if patient outcomes were used as features during model training. For instance, including future diagnoses or treatment outcomes as predictors for current disease prediction would lead to inflated model performance during evaluation. However, in real-world scenarios, this information would not be available when making predictions. This leakage would cause the model to falsely appear highly accurate but fail to generalize to new cases. To prevent data leakage, only historical and relevant patient data should be used, excluding any information that reflects outcomes occurring after the prediction target, ensuring realistic model performance estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658db0de-18c4-46bb-9745-4236e096c42b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4b393d8-d9e2-439d-bb41-08ac372c8428",
   "metadata": {},
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?\n",
    "\n",
    "Ans. Preventing data leakage is crucial when building a machine learning model to ensure its reliability and generalization. Here are several steps we can take to prevent data leakage:\n",
    "\n",
    "1. We can gain a deep understanding of the dataset, including the meaning and source of each feature, as well as the problem we are trying to solve. Identify any potential sources of leakage, such as features that include future information or information not available at prediction time.\n",
    "\n",
    "2. We should divide our dataset into distinct training, validation, and test sets. Ensure that all temporal or sequential aspects are preserved when splitting data. For time series data, use chronological splits to mimic the real-world scenario.\n",
    "\n",
    "3. We can exclude features that contain information from the future or target leakage.\n",
    "\n",
    "4. When using techniques like cross-validation for model evaluation, make sure to set it up properly to avoid leakage. For example, in time series data, use time-based splits and avoid shuffling the data.\n",
    "\n",
    "6. Carefully handle missing data. Imputing missing values with future information can lead to leakage, so choose imputation methods based on historical data.\n",
    "\n",
    "7. Regularization Techniques\n",
    "\n",
    "9. Collaborate with domain experts who have a deep understanding of the problem domain to identify potential sources of leakage and implement appropriate safeguards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d0c52-97ae-4ec0-aac1-0a468212d837",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e8aad3f-ccb4-461e-9711-b692ba99c610",
   "metadata": {},
   "source": [
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "\n",
    "Ans. A confusion matrix is a table or matrix used to evaluate the performance of a classification model, such as a logistic regression model. It provides a detailed breakdown of the model's predictions compared to the actual outcomes for a given dataset. A confusion matrix consists of four key components: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These components are used to compute various performance metrics for the model. Here's what each part of the confusion matrix represents:\n",
    "\n",
    "1. **True Positives (TP)**: These are instances where the model correctly predicted the positive class. In binary classification, TP represents cases where the model correctly identified the presence of the condition or event.\n",
    "\n",
    "2. **True Negatives (TN)**: These are instances where the model correctly predicted the negative class. In binary classification, TN represents cases where the model correctly identified the absence of the condition or event.\n",
    "\n",
    "3. **False Positives (FP)**: These are instances where the model incorrectly predicted the positive class when the actual class was negative. FP is also known as a Type I error.\n",
    "\n",
    "4. **False Negatives (FN)**: These are instances where the model incorrectly predicted the negative class when the actual class was positive. FN is also known as a Type II error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47332c9-a7c4-44b3-af57-54330dfa66e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "772bbeec-aaf6-4197-8080-62ad9b412055",
   "metadata": {},
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n",
    "\n",
    "Ans. The difference between precision and recall in the context of a confusion matrix:\n",
    "\n",
    "\n",
    "|Precision|Recall|\n",
    "|---|---|\n",
    "|Out of all actual values, how many are correctly predicted| Out of all predicted values, how many are correctly predicted with actual values|\n",
    "|When FP is more important, we use Precision| When FN is more important, we use Recall|\n",
    "| Precision : $$ \\frac{TP}{TP+FP} $$|Recall :$$ \\frac{TP}{TP+FP} $$|\n",
    "\n",
    "\n",
    "where,\n",
    "- TP : True Positive\n",
    "- FP : False Positive\n",
    "- TN : True Negative\n",
    "- FN : False Negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c1439-fb25-42dc-9914-03439a730c9b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4bc8b3a-7e2b-4194-a9c4-436470e89a0f",
   "metadata": {},
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n",
    "\n",
    "Ans. By analyzing the four components of the confusion matrix (True Positives, True Negatives, False Positives, and False Negatives), we can gain insights into the model's strengths and weaknesses. \n",
    "\n",
    "1. **True Positives (TP)**: These are cases where the model correctly predicted the positive class, and the actual outcome was indeed positive. In medical diagnostics, for example, TP represents correctly diagnosed cases of a disease.\n",
    "\n",
    "2. **True Negatives (TN)**: These are cases where the model correctly predicted the negative class, and the actual outcome was indeed negative. TN represents correctly identified non-events or non-occurrences.\n",
    "\n",
    "3. **False Positives (FP)**: These are cases where the model incorrectly predicted the positive class when the actual outcome was negative. FP is also known as a Type I error. It represents instances where the model falsely detected the presence of a condition or event when it wasn't actually present. In medical diagnostics, this could be a false alarm or a false positive test result.\n",
    "\n",
    "4. **False Negatives (FN)**: These are cases where the model incorrectly predicted the negative class when the actual outcome was positive. FN is also known as a Type II error. It represents instances where the model failed to detect the presence of a condition or event when it was actually present. In medical diagnostics, this could be a missed diagnosis or a false negative test result.\n",
    "\n",
    "Interpreting the confusion matrix involves understanding the implications of these different types of errors in the context of our specific problem or application:\n",
    "\n",
    "- **Precision**: Precision is a measure of the model's ability to make positive predictions correctly. It is calculated as TP / (TP + FP). A high precision indicates that the model makes few false positive errors.\n",
    "\n",
    "- **Recall (Sensitivity)**: Recall is a measure of the model's ability to correctly identify all actual positive cases. It is calculated as TP / (TP + FN). A high recall indicates that the model makes few false negative errors.\n",
    "\n",
    "- **Specificity**: Specificity is a measure of the model's ability to correctly identify all actual negative cases. It is calculated as TN / (TN + FP). A high specificity indicates that the model makes few false positive errors in the negative class.\n",
    "\n",
    "- **F1-Score**: The F1-score is the harmonic mean of precision and recall and provides a balanced measure of a model's performance, considering both false positives and false negatives.\n",
    "\n",
    "Analyzing these metrics along with the confusion matrix can help us understand the trade-offs between different types of errors. For example, in a medical diagnosis scenario, if false positives have serious consequences (e.g., unnecessary treatments), we may want to prioritize high precision even if it means lower recall. Conversely, if false negatives are more critical (e.g., missing a life-threatening disease), we may prioritize high recall at the expense of precision.\n"
   ]
  },
  {
   "attachments": {
    "021d869d-fd91-4033-bc77-a08c994c44d4.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "656fb9d2-8fb4-4f8a-b1d8-ab849732e531",
   "metadata": {},
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?\n",
    "\n",
    "Ans. The confusion matrix is as shown:\n",
    "\n",
    "![image.png](attachment:021d869d-fd91-4033-bc77-a08c994c44d4.png)\n",
    "\n",
    "where,\n",
    "- TP : True Positive\n",
    "- FP : False Positive\n",
    "- TN : True Negative\n",
    "- FN : False Negative\n",
    "\n",
    "We can derive following metrics:\n",
    "\n",
    "- Model Accuracy : $$ \\frac{TP+TN}{TP+TN+FP+FN} $$\n",
    "\n",
    "- Precision : $$ \\frac{TP}{TP+FP} $$\n",
    "\n",
    "- Recall (aka Sensitivity) : $$ \\frac{TP}{TP+FN} $$\n",
    "\n",
    "- Specificity : $$ \\frac{TN}{TN+FP} $$\n",
    "\n",
    "- F-beta score : $$ (1+\\beta^2) \\frac{\\text{Precision}*\\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n",
    "\n",
    "    - if FP and FN both are important, $\\beta=1$\n",
    "    - if FP is more important, $\\beta=0.5$\n",
    "    - if FN is more important, $\\beta=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b501396-33f9-4f65-8a90-47304bb888e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1759bd68-af32-42ae-a465-274633e5ff86",
   "metadata": {},
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
    "\n",
    "Ans. The relationship between the accuracy of a model and the values in its confusion matrix is as follows:\n",
    "Model Accuracy : $$ \\frac{TP+TN}{TP+TN+FP+FN} $$\n",
    "\n",
    "where,\n",
    "- TP : True Positive\n",
    "- FP : False Positive\n",
    "- TN : True Negative\n",
    "- FN : False Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb59f8cf-2703-4df8-9b51-cd9365e4ba95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0eeb86d0-143b-4089-984a-cd243157f6b3",
   "metadata": {},
   "source": [
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model?\n",
    "\n",
    "Ans. A confusion matrix can be a valuable tool for identifying potential biases or limitations in our machine learning model:\n",
    "\n",
    "1. **Class Imbalance Bias**: If one class is much smaller than the other, the model might be biased towards the majority class, resulting in high accuracy but poor performance on the minority class.\n",
    "\n",
    "2. **False Positive vs. False Negative Bias**: False positives (Type I errors) might indicate that the model is overly sensitive and is making predictions in cases where it shouldn't. False negatives (Type II errors) might indicate that the model is conservative and is failing to make predictions in cases where it should.\n",
    "\n",
    "5. **Evaluation Metrics**: We can use evaluation metrics such as precision, recall, F1-score, and specificity to gain a more nuanced understanding of your model's performance. Evaluate these metrics not only for the overall dataset but also for each class and subgroup to uncover biases or limitations.\n",
    "\n",
    "7. **Sampling or Data Collection Bias**: Sampling bias or data collection bias can lead to model limitations and inaccuracies, particularly if certain groups or scenarios are underrepresented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29ae23-bc3d-403a-970b-29352f4bc6d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
