{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5956c700-3e18-4abc-92e7-20a5ab0c0dec",
   "metadata": {},
   "source": [
    "# Boosting - 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc938637-acdb-426e-9a4c-73dd845ee7ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b69695b3-1cad-4ca6-a949-6c649a839cb3",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Ans. Gradient Boosting Regression is a machine learning technique used for regression tasks, where the goal is to predict a continuous numerical value, such as a price, temperature, or a score. It belongs to the ensemble learning methods, which means it combines the predictions of multiple base machine learning models to produce a more accurate and robust final prediction.\n",
    "\n",
    "Here's a brief overview of how Gradient Boosting Regression works:\n",
    "\n",
    "1. **Boosting:** Gradient Boosting is a boosting algorithm, which means it builds an ensemble of weak learners (typically decision trees) sequentially. Each new learner is trained to correct the errors made by the previous ones. This is in contrast to bagging techniques like Random Forests, where each learner is trained independently.\n",
    "\n",
    "2. **Gradient Descent:** The \"gradient\" in Gradient Boosting refers to the gradient of a loss function. In regression, the loss function is often mean squared error (MSE), which measures the average squared difference between the predicted and actual values. Gradient Boosting tries to minimize this loss by iteratively fitting new models to the negative gradient of the loss with respect to the previous predictions. This process gradually reduces the residual error.\n",
    "\n",
    "3. **Weak Learners:** In the context of Gradient Boosting Regression, the weak learners are typically shallow decision trees, often called \"stumps\" or \"shallow trees.\" These trees are usually simple and have a limited depth, often just a single split or a few splits.\n",
    "\n",
    "4. **Weighted Combination:** After each weak learner is trained, it is assigned a weight based on how much it contributes to reducing the loss function. The final prediction is a weighted combination of the predictions of all the weak learners. The weights ensure that more accurate models have a greater influence on the final prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287807c1-0139-45a8-81ba-ff32e9633546",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb55cb8c-5002-40da-b064-3c289010f4d1",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c06cd422-924d-4153-b38e-87dbbe1cde28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.488135</td>\n",
       "      <td>9.811120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.151894</td>\n",
       "      <td>15.204614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.027634</td>\n",
       "      <td>12.520930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.448832</td>\n",
       "      <td>9.361420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.236548</td>\n",
       "      <td>9.961348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.831914</td>\n",
       "      <td>4.345422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.865129</td>\n",
       "      <td>10.926849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.201075</td>\n",
       "      <td>-0.287399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>8.289400</td>\n",
       "      <td>16.123268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.046955</td>\n",
       "      <td>0.111389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X          y\n",
       "0   5.488135   9.811120\n",
       "1   7.151894  15.204614\n",
       "2   6.027634  12.520930\n",
       "3   5.448832   9.361420\n",
       "4   4.236548   9.961348\n",
       "..       ...        ...\n",
       "95  1.831914   4.345422\n",
       "96  5.865129  10.926849\n",
       "97  0.201075  -0.287399\n",
       "98  8.289400  16.123268\n",
       "99  0.046955   0.111389\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a simple dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X.squeeze() + np.random.randn(100)\n",
    "\n",
    "df=pd.DataFrame({'X':X.reshape(1,100)[0], 'y':y})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc1133a7-2d35-4c7a-a9b1-b29ca891097e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f19f829fdf0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61b62248-4610-493e-9d9a-7bac345ff2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.61\n",
      "R-squared: 0.98\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the number of boosting iterations (weak learners)\n",
    "n_estimators = 100\n",
    "\n",
    "# Initialize the ensemble prediction with the mean of the target values\n",
    "F = np.mean(y) * np.ones(len(y))\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Create the weak learners (decision trees)\n",
    "weak_learners = []\n",
    "for i in range(n_estimators):\n",
    "    # Calculate the negative gradient (residuals)\n",
    "    residuals = y - F\n",
    "    \n",
    "    # Fit a weak learner (decision tree) to the residuals\n",
    "    weak_learner = DecisionTreeRegressor(max_depth=1)\n",
    "    weak_learner.fit(X, residuals)\n",
    "    \n",
    "    # Update the ensemble prediction with the weighted predictions of the weak learner\n",
    "    F += learning_rate * weak_learner.predict(X)\n",
    "    \n",
    "    # Store the weak learner in the list\n",
    "    weak_learners.append(weak_learner)\n",
    "\n",
    "# Make predictions with the final ensemble\n",
    "y_pred = F\n",
    "\n",
    "# Calculate MSE and R²\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c36a2-e6b3-4be6-874a-c47201b705e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a53fe73a-a240-42ec-b3ad-759f76f1d351",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a9cfd78-9e8c-48b5-afc4-205ab2da8e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1, 0.2],\n",
       "                         &#x27;max_depth&#x27;: [1, 2, 3],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 150]},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1, 0.2],\n",
       "                         &#x27;max_depth&#x27;: [1, 2, 3],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 150]},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 0.2],\n",
       "                         'max_depth': [1, 2, 3],\n",
       "                         'n_estimators': [50, 100, 150]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a simple dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X.squeeze() + np.random.randn(100)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Create a Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Perform a grid search with cross-validation\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f4a7bb-13c0-4161-9401-9861ef0d1ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a Gradient Boosting Regressor with the best hyperparameters\n",
    "best_gb_regressor = GradientBoostingRegressor(**best_params)\n",
    "\n",
    "# Fit the best model on the training data\n",
    "best_gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = best_gb_regressor.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "757ac831-7526-4da2-a1de-8d6de255a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 50}\n",
      "Validation Mean Squared Error: 1.32\n",
      "Validation R-squared: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Calculate MSE and R² on the validation set\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(f\"Validation Mean Squared Error: {mse_val:.2f}\")\n",
    "print(f\"Validation R-squared: {r2_val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e795d-a247-4f12-adae-54a08ed60133",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90d96f2f-da0c-4c58-a099-2bb9a6bc5373",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "Ans.  In the context of Gradient Boosting Regression, the weak learners are typically shallow decision trees, often called \"stumps\" or \"shallow trees.\" These trees are usually simple and have a limited depth, often just a single split or a few splits.  After each weak learner is trained, it is assigned a weight based on how much it contributes to reducing the loss function. The final prediction is a weighted combination of the predictions of all the weak learners. The weights ensure that more accurate models have a greater influence on the final prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18768973-2868-49f2-b6c4-e9adbe8be324",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c84dcfe-0260-48bd-bea5-ecec398a4bd0",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "Ans. Here's the intuition step by step:\n",
    "\n",
    "1. **Initialization**: Gradient Boosting starts with an initial prediction, often a simple one like the mean of the target values. This serves as the starting point.\n",
    "\n",
    "2. **Error Calculation**: The algorithm calculates the error, which is the difference between the current predictions and the actual target values. It uses a loss function, such as mean squared error (MSE), to quantify this error.\n",
    "\n",
    "3. **Gradient Calculation**: The gradient of the loss function is computed with respect to the current predictions. This gradient tells us how the loss would change if we made small adjustments to our predictions. Essentially, it indicates the direction and magnitude of the error reduction we need.\n",
    "\n",
    "4. **Update Predictions**: The next weak learner (decision tree) is trained to predict the negative gradient. This means it tries to find patterns in the data that help reduce the error in the direction indicated by the gradient.\n",
    "\n",
    "5. **Weighted Addition**: The predictions from the new weak learner are added to the current predictions with a certain weight. The weight is determined by how much this weak learner has reduced the error. This step ensures that the overall model moves closer to the true target values.\n",
    "\n",
    "6. **Iteration**: Steps 2-5 are repeated for a specified number of iterations (or until a stopping criterion is met). With each iteration, the model becomes better at reducing the error, as each new weak learner focuses on the remaining errors.\n",
    "\n",
    "7. **Final Prediction**: The final prediction is the sum of the predictions from all the weak learners. Since each learner is trained to correct the errors of the previous ones, the ensemble gradually approaches an accurate prediction of the target variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2fe0be-5451-42c8-aed8-e721339f5b26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09f45835-6920-42f3-9d04-f5f6ef84d2b2",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Ans. The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential and adaptive manner. The key idea is to train each weak learner to correct the errors made by the previous ones. Here's how the ensemble is constructed step by step:\n",
    "\n",
    "1. **Initialization**: The ensemble starts with an initial prediction, which is typically a simple estimate for the target variable. Common choices include the mean of the target values for regression tasks or a balanced class distribution for classification tasks.\n",
    "\n",
    "2. **Compute Residuals**: After the initial prediction, the algorithm computes the residuals, which are the differences between the actual target values and the current predictions. These residuals represent the errors made by the current ensemble.\n",
    "\n",
    "3. **Train a Weak Learner**: A new weak learner (usually a shallow decision tree) is trained on the residuals from the previous step. The goal of this weak learner is to capture patterns in the data that can help reduce the errors made by the current ensemble.\n",
    "\n",
    "4. **Update Predictions**: The predictions from the newly trained weak learner are added to the current predictions with a certain weight. The weight is determined during the training process and is often calculated using techniques like gradient descent. The weight ensures that the contribution of each weak learner is proportional to its ability to reduce the error.\n",
    "\n",
    "5. **Repeat**: Steps 2-4 are repeated for a specified number of iterations or until a stopping criterion is met. With each iteration, a new weak learner is trained to correct the errors of the current ensemble. The process continues until the ensemble's performance reaches a satisfactory level or no further improvement is observed.\n",
    "\n",
    "The ensemble of weak learners is combined to form the final prediction. Each weak learner contributes to the prediction based on its ability to reduce the errors made by the previous ones. The final prediction is the sum (in regression) or a weighted vote (in classification) of the individual weak learner predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4131def-2099-444c-ae64-66f3ca9dd59b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d75e69ee-27cc-487e-b5a8-7e09d5b40145",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "\n",
    "Ans. Here are the key steps and concepts involved in building the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "1. **Loss Function**: Gradient Boosting begins with defining a loss function, typically denoted as L(y, F(x)), where y represents the true target values, and F(x) represents the current ensemble's predictions. The goal is to minimize this loss function.\n",
    "\n",
    "2. **Initial Prediction**: The ensemble starts with an initial prediction, often represented as F₀(x). This is typically a simple constant, such as the mean of the target values for regression problems or a probability distribution for classification problems.\n",
    "\n",
    "3. **Residual Calculation**: At each iteration, the algorithm calculates the residuals (denoted as r) by subtracting the current prediction from the true target values: rᵢ = yᵢ - Fᵢ₋₁(xᵢ), where i represents the data point at index i, and Fᵢ₋₁(xᵢ) is the prediction made by the current ensemble at that point.\n",
    "\n",
    "4. **Fit a Weak Learner**: A new weak learner (usually a decision tree) is trained to predict the residuals. This is done by fitting a model Gᵢ(x) to the residual values r.\n",
    "\n",
    "5. **Update Ensemble**: The prediction of the ensemble is updated by adding the prediction of the newly trained weak learner, scaled by a learning rate (η): Fᵢ(x) = Fᵢ₋₁(x) + η * Gᵢ(x). This step ensures that the ensemble is moving closer to the true target values by adjusting its predictions based on the errors made so far.\n",
    "\n",
    "6. **Iterate**: Steps 3-5 are repeated for a specified number of iterations or until a stopping criterion is met. In each iteration, a new weak learner is trained to predict the residuals, and the ensemble is updated.\n",
    "\n",
    "7. **Final Prediction**: The final prediction is the sum of the predictions from all the weak learners: F(x) = F₀(x) + η * G₁(x) + η * G₂(x) + ... + η * Gₙ(x), where n is the total number of iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba35c0a-548f-4cb7-b433-e4ce28689d3b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
