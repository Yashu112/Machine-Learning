{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a038542e-50af-4490-8ae9-5bcb1d297d55",
   "metadata": {},
   "source": [
    "# PCA-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1f08c3-25e1-4c97-b29b-f25a557fe095",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fbb15a0-d3a1-4350-8dfe-caf793b99039",
   "metadata": {},
   "source": [
    "Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach?\n",
    "Explain with an example.\n",
    "\n",
    "Ans. Eigenvalues and eigenvectors are mathematical concepts that play a crucial role in various fields, including linear algebra, physics, and machine learning. They are intimately related to the eigen-decomposition (also known as spectral decomposition) approach, which is a method for diagonalizing certain types of matrices. Let's explore these concepts with an example.\n",
    "\n",
    "**Eigenvalues**:\n",
    "\n",
    "An eigenvalue (λ) of a square matrix (A) is a scalar value that represents how the matrix stretches or compresses space in a particular direction when multiplied by a vector. Mathematically, it can be defined as:\n",
    "\n",
    "$$A * v = \\lambda * v$$\n",
    "\n",
    "Here, λ is the eigenvalue, A is the matrix, and v is the eigenvector associated with λ.\n",
    "\n",
    "**Eigenvectors**:\n",
    "\n",
    "An eigenvector (v) of a matrix (A) is a non-zero vector that remains in the same direction (up to a scalar multiple) when multiplied by the matrix. Mathematically, it can be defined as:\n",
    "\n",
    "$$A * v = \\lambda * v$$\n",
    "\n",
    "Here, v is the eigenvector, λ is the corresponding eigenvalue, and A is the matrix.\n",
    "\n",
    "**Eigen-Decomposition**:\n",
    "\n",
    "Eigen-decomposition is a process that decomposes a square matrix (A) into the product of three matrices: P, D, and P⁻¹, where:\n",
    "\n",
    "- P is a matrix whose columns are the eigenvectors of A.\n",
    "- D is a diagonal matrix whose diagonal elements are the eigenvalues of A.\n",
    "- P⁻¹ is the inverse of the matrix P.\n",
    "\n",
    "Mathematically, it can be represented as:\n",
    "\n",
    "A = P * D * P⁻¹\n",
    "\n",
    "**Example**:\n",
    "\n",
    "Let's illustrate these concepts with a simple example:\n",
    "\n",
    "Consider the following 2x2 matrix A:\n",
    "\n",
    "```\n",
    "A = | 3  1 |\n",
    "    | 1  2 |\n",
    "```\n",
    "\n",
    "To find the eigenvalues and eigenvectors of A, we solve the eigenvalue equation:\n",
    "\n",
    "$$A * v = \\lambda * v$$\n",
    "\n",
    "First, we calculate the eigenvalues by solving the characteristic equation:\n",
    "\n",
    "$$\\det(A - \\lambda * I) = 0$$\n",
    "\n",
    "Where I is the identity matrix. So,\n",
    "\n",
    "```\n",
    "| 3-λ  1   |  \n",
    "| 1    2-λ |  = 0\n",
    "```\n",
    "\n",
    "Expanding the determinant, we get:\n",
    "\n",
    "(3-λ)(2-λ) - (1)(1) = 0\n",
    "\n",
    "Solving this quadratic equation gives us the eigenvalues:\n",
    "\n",
    "λ₁ = 4\n",
    "λ₂ = 1\n",
    "\n",
    "Now that we have the eigenvalues, we can find the corresponding eigenvectors by solving the equation A * v = λ * v for each eigenvalue.\n",
    "\n",
    "For λ₁ = 4:\n",
    "```\n",
    "(3-4)v₁ + v₂ = 0\n",
    "-v₁ + v₂ = 0\n",
    "v₁ = v₂\n",
    "```\n",
    "\n",
    "So, one eigenvector corresponding to λ₁ is [1, 1].\n",
    "\n",
    "For λ₂ = 1:\n",
    "```\n",
    "(3-1)v₁ + v₂ = 0\n",
    "2v₁ + v₂ = 0\n",
    "v₂ = -2v₁\n",
    "```\n",
    "\n",
    "So, one eigenvector corresponding to λ₂ is [1, -2].\n",
    "\n",
    "The matrix P is formed by stacking these eigenvectors as columns:\n",
    "```\n",
    "P = | 1   1 |\n",
    "    | 1  -2 |\n",
    "```\n",
    "\n",
    "The diagonal matrix D contains the eigenvalues on its diagonal:\n",
    "```\n",
    "D = | 4   0 |\n",
    "    | 0   1 |\n",
    "```\n",
    "\n",
    "The inverse of matrix P, P⁻¹, is:\n",
    "```\n",
    "P⁻¹ = |  2/3   1/3 |\n",
    "      |  1/3  -1/3 |\n",
    "```\n",
    "\n",
    "So, the eigen-decomposition of matrix A is:\n",
    "```\n",
    "A = P * D * P⁻¹\n",
    "\n",
    "| 3  1 | = | 1   1 |   | 4   0 |   |  2/3   1/3 |\n",
    "           | 1   2 |   | 1  -2 |   |  1/3  -1/3 |\n",
    "```\n",
    "\n",
    "This decomposition allows us to express matrix A in terms of its eigenvalues and eigenvectors, which can have various applications in linear algebra, data analysis, and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e33539-a756-4751-9f98-92605888a0ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1df44515-0b42-420f-a3cf-87e77dcc5b08",
   "metadata": {},
   "source": [
    "Q2. What is eigen decomposition and what is its significance in linear algebra?\n",
    "\n",
    "Ans. Eigen-decomposition is a process that decomposes a square matrix (A) into the product of three matrices: P, D, and P⁻¹, where:\n",
    "\n",
    "- P is a matrix whose columns are the eigenvectors of A.\n",
    "- D is a diagonal matrix whose diagonal elements are the eigenvalues of A.\n",
    "- P⁻¹ is the inverse of the matrix P.\n",
    "\n",
    "Mathematically, it can be represented as:\n",
    "\n",
    "A = P * D * P⁻¹\n",
    "\n",
    "This decomposition allows us to express matrix A in terms of its eigenvalues and eigenvectors, which can have various applications in linear algebra, data analysis, and machine learning. For example, this approach is used in Principal Component Analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b7c38-7480-493e-8d1e-cc2a8c19ce78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd053ae0-1af2-47d8-99ad-39d351749b2f",
   "metadata": {},
   "source": [
    "Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the\n",
    "Eigen-Decomposition approach? Provide a brief proof to support your answer.\n",
    "\n",
    "Ans. For a square matrix to be diagonalizable using the Eigen-Decomposition approach, it must satisfy certain conditions. Specifically, the matrix must be diagonalizable if and only if it meets the following criteria:\n",
    "\n",
    "1. **Matrix Must Be Square**: The matrix must be a square matrix, meaning it has the same number of rows and columns.\n",
    "\n",
    "2. **Linearly Independent Eigenvectors**: The matrix must have a set of linearly independent eigenvectors that span the entire vector space. In other words, there must be enough linearly independent eigenvectors to form a complete basis for the vector space.\n",
    "\n",
    "3. **Sufficiently Many Eigenvectors**: The matrix must have as many linearly independent eigenvectors as its size (number of rows or columns) to fill the entire basis. If it has fewer linearly independent eigenvectors than its size, it is not diagonalizable.\n",
    "\n",
    "Let's provide a brief proof to support these conditions:\n",
    "\n",
    "**Proof**:\n",
    "\n",
    "1. **Matrix Must Be Square**:\n",
    "   \n",
    "   - To diagonalize a matrix, you need to compute the eigenvalues and eigenvectors. The eigenvalues are calculated as solutions to the characteristic equation, and eigenvectors correspond to each eigenvalue. This process requires the matrix to be square, as non-square matrices do not have eigenvalues or eigenvectors in the traditional sense.\n",
    "\n",
    "2. **Linearly Independent Eigenvectors**:\n",
    "   \n",
    "   - Suppose we have an n x n matrix A with n linearly independent eigenvectors (v₁, v₂, ..., vn) corresponding to distinct eigenvalues (λ₁, λ₂, ..., λn). \n",
    "\n",
    "   - To form the matrix P (consisting of eigenvectors) in the Eigen-Decomposition, these eigenvectors must be linearly independent. If they are linearly independent, they span the entire vector space Rⁿ.\n",
    "\n",
    "   - If the eigenvectors were not linearly independent, they would not be able to form a basis for Rⁿ, and A could not be diagonalized.\n",
    "\n",
    "3. **Sufficiently Many Eigenvectors**:\n",
    "   \n",
    "   - Suppose we have an n x n matrix A with k linearly independent eigenvectors (where k < n). In this case, we cannot create a matrix P with n linearly independent eigenvectors because there are not enough linearly independent eigenvectors available.\n",
    "\n",
    "   - Since we cannot form a full basis for Rⁿ, it's impossible to construct a matrix P with n linearly independent eigenvectors, which means A is not diagonalizable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992548b0-e9f6-4684-8368-90ab99c3e09a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5e9069a-f74f-4c8a-a893-27846b7cad73",
   "metadata": {},
   "source": [
    "Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach?\n",
    "How is it related to the diagonalizability of a matrix? Explain with an example.\n",
    "\n",
    "Ans. The spectral theorem is a fundamental result in linear algebra and functional analysis that has significant implications in the context of the Eigen-Decomposition approach. It provides conditions under which a matrix can be diagonalized, and it establishes the relationship between diagonalizability and certain properties of the matrix. Let's explore the significance of the spectral theorem in the context of Eigen-Decomposition and provide an example.\n",
    "\n",
    "**Significance of the Spectral Theorem**:\n",
    "\n",
    "The spectral theorem states that for a **symmetric** or **Hermitian** matrix (a square matrix that is equal to its conjugate transpose), there exists an orthonormal basis of eigenvectors. In other words, the spectral theorem tells us that under certain conditions, it is always possible to find a set of linearly independent eigenvectors that form an orthonormal basis for the vector space.\n",
    "\n",
    "In the context of the Eigen-Decomposition approach, the spectral theorem is highly significant for several reasons:\n",
    "\n",
    "1. **Diagonalizability**: The spectral theorem guarantees that for symmetric or Hermitian matrices, diagonalization is always possible. That is, these matrices can always be expressed as the product of their eigenvectors and a diagonal matrix of their eigenvalues, which is precisely what the Eigen-Decomposition method aims to achieve.\n",
    "\n",
    "2. **Orthogonality of Eigenvectors**: The spectral theorem ensures that the eigenvectors corresponding to distinct eigenvalues are orthogonal to each other. This orthogonality simplifies the process of constructing the matrix of eigenvectors (P) and its inverse because the columns of P form an orthonormal basis.\n",
    "\n",
    "3. **Physical Interpretation**: In many applications, such as physics and engineering, symmetric matrices often represent observables, and their eigenvalues correspond to physical quantities. The spectral theorem provides a rigorous mathematical foundation for interpreting these eigenvalues and eigenvectors.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "Let's consider a simple example to illustrate the significance of the spectral theorem in the context of Eigen-Decomposition. Suppose we have the following symmetric matrix A:\n",
    "\n",
    "```\n",
    "A = | 4  2 |\n",
    "    | 2  5 |\n",
    "```\n",
    "\n",
    "1. **Eigenvalues and Eigenvectors**:\n",
    "\n",
    "   We can calculate the eigenvalues and eigenvectors of A. The eigenvalues are λ₁ = 3 and λ₂ = 6, and the corresponding normalized eigenvectors are:\n",
    "\n",
    "   For λ₁ = 3:\n",
    "   Eigenvector v₁ = [1/$\\sqrt5$, -2/$\\sqrt5$]\n",
    "\n",
    "   For λ₂ = 6:\n",
    "   Eigenvector v₂ = [2/$\\sqrt5$, 1/$\\sqrt5$]\n",
    "\n",
    "2. **Orthonormal Eigenvectors**:\n",
    "\n",
    "   Notice that the eigenvectors v₁ and v₂ are orthogonal to each other (their dot product is zero) and are normalized (their lengths are 1). This confirms that they form an orthonormal basis for R².\n",
    "\n",
    "3. **Diagonalization**:\n",
    "\n",
    "   Using the eigenvectors and eigenvalues, we can construct the matrix P of eigenvectors:\n",
    "\n",
    "P= | 1/$\\sqrt5$   -2/$\\sqrt5$ |\n",
    "\n",
    "   | 2/$\\sqrt5$  1/$\\sqrt5$ |\n",
    "\n",
    "\n",
    "   We can also form the diagonal matrix D of eigenvalues:\n",
    "\n",
    "   ```\n",
    "   D = | 3   0 |\n",
    "       | 0   6 |\n",
    "   ```\n",
    "\n",
    "   With these matrices, we can express A as the Eigen-Decomposition:\n",
    "\n",
    "   ```\n",
    "   A = P * D * P⁻¹\n",
    "   ```\n",
    "\n",
    "   Where P⁻¹ is the inverse of matrix P.\n",
    "\n",
    "In this example, the spectral theorem guarantees that we can find orthonormal eigenvectors for the symmetric matrix A, which enables us to diagonalize A using the Eigen-Decomposition approach. This diagonalization simplifies various calculations and provides insights into the original matrix's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0edb2-2ab0-4b2a-bd79-68f3111003b7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c9684bc-464a-41c5-90ee-a87ead2ea256",
   "metadata": {},
   "source": [
    "Q5. How do you find the eigenvalues of a matrix and what do they represent?\n",
    "\n",
    "Ans. To calculate the eigenvalues of a matrix, we solve the characteristic equation:\n",
    "\n",
    "$$\\det(A - \\lambda * I) = 0 $$\n",
    "\n",
    "Where I is the identity matrix.\n",
    "\n",
    "**Eigenvalues**: An eigenvalue (λ) of a square matrix (A) is a scalar value that represents how the matrix stretches or compresses space in a particular direction when multiplied by a vector. Mathematically, it can be defined as:\n",
    "\n",
    "$$A * v = \\lambda * v$$\n",
    "\n",
    "Here, λ is the eigenvalue, A is the matrix, and v is the eigenvector associated with λ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89fb697-3c0f-4de4-b8ab-cdc00935ae26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37114960-6ed2-46f8-bf79-ead991f4dac2",
   "metadata": {},
   "source": [
    "Q6. What are eigenvectors and how are they related to eigenvalues?\n",
    "\n",
    "Ans. An eigenvector (v) of a matrix (A) is a non-zero vector that remains in the same direction (up to a scalar multiple) when multiplied by the matrix. Mathematically, the realtion between Eigen values and Eigen vectors is as follows:\n",
    "\n",
    "$$A * v = \\lambda * v$$\n",
    "\n",
    "Here, v is the eigenvector, λ is the corresponding eigenvalue, and A is the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb565332-bde4-4cfe-8638-2ba713619e65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a5ae680-742a-4073-9465-c6b2693fe95f",
   "metadata": {},
   "source": [
    "Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?\n",
    "\n",
    "Ans. Eigenvectors and eigenvalues have important geometric interpretations that help us understand their significance in linear transformations and matrix operations. Let's explore the geometric interpretation of eigenvectors and eigenvalues:\n",
    "\n",
    "**Eigenvectors**:\n",
    "\n",
    "An eigenvector of a square matrix represents a direction in the vector space that remains unchanged in direction (up to a scalar multiple) when the matrix is applied as a linear transformation. Geometrically, eigenvectors point along these special directions and are stretched or compressed but not rotated by the matrix transformation.\n",
    "\n",
    "Here are some key points regarding the geometric interpretation of eigenvectors:\n",
    "\n",
    "1. **Direction Preservation**: An eigenvector v of a matrix A is a non-zero vector such that when A is applied to v (i.e., A * v), the resulting vector is parallel to v. In other words, v points in a direction that is preserved by the matrix transformation.\n",
    "\n",
    "2. **Scaling Factor**: The eigenvalue corresponding to an eigenvector v (denoted as λ) represents the factor by which the eigenvector is scaled during the transformation. If λ is positive, the eigenvector is stretched; if λ is negative, it is flipped and stretched (reflected); if λ is zero, the eigenvector is compressed to a point.\n",
    "\n",
    "3. **Eigenvalue Magnitude**: The magnitude (absolute value) of the eigenvalue λ determines the extent of stretching or compression. Larger absolute values of λ indicate stronger stretching or compression along the eigenvector direction.\n",
    "\n",
    "4. **Linear Independence**: Eigenvectors corresponding to different eigenvalues are linearly independent and point in different, non-parallel directions. This property allows for a diagonalization of the matrix.\n",
    "\n",
    "**Eigenvalues**:\n",
    "\n",
    "Eigenvalues represent the scaling factors associated with the eigenvectors of a matrix. Geometrically, eigenvalues indicate how much the space is stretched or compressed along the corresponding eigenvector directions.\n",
    "\n",
    "1. **Scaling Along Eigenvector Directions**: Each eigenvalue λ corresponds to a specific eigenvector direction. The eigenvalue indicates how much the matrix transformation scales (stretches or compresses) along that direction.\n",
    "\n",
    "2. **Magnitude and Sign**: The magnitude (absolute value) of an eigenvalue |λ| represents the factor by which the space is scaled along the corresponding eigenvector. The sign of λ determines whether the transformation includes a reflection (if λ is negative) or simply stretching/compression (if λ is positive).\n",
    "\n",
    "3. **Relation to Matrix Determinant**: The product of all eigenvalues of a matrix equals the determinant of the matrix. This relates the eigenvalues to the overall scaling factor of the transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90222cf9-29cc-4ee0-9b03-a4f0e523a009",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2443603-2630-40b8-a8bb-4ece1730fc25",
   "metadata": {},
   "source": [
    "Q8. What are some real-world applications of eigen decomposition?\n",
    "\n",
    "Ans. Here are some real-world applications of eigen decomposition:\n",
    "\n",
    "1. **Principal Component Analysis (PCA)**: PCA is a dimensionality reduction technique that uses eigen decomposition to identify the principal components (eigenvectors) of a dataset. Applications: Data compression, data visualization, noise reduction, and feature selection in data analysis, image and video compression.\n",
    "\n",
    "2. **Quantum Mechanics**: In quantum mechanics, the eigen decomposition of a Hamiltonian matrix is used to find energy levels and corresponding wave functions of quantum systems. Applications: Understanding the behavior of quantum systems, predicting energy levels of molecules and atoms.\n",
    "\n",
    "3. **Vibrations and Structural Analysis**: Eigen decomposition is employed to analyze the modes of vibration and natural frequencies of mechanical and structural systems.\n",
    "\n",
    "4. **Recommendation Systems**: Eigen decomposition is used in collaborative filtering recommendation algorithms, where it helps uncover latent factors in user-item interaction matrices. Applications: Personalized recommendations in e-commerce, movie recommendations, content recommendations.\n",
    "\n",
    "5. **Network Analysis**: Eigen decomposition of adjacency matrices or Laplacian matrices of networks can reveal important properties and structures in networks. Applications: Identifying communities in social networks, analyzing transportation networks, detecting anomalies in cybersecurity.\n",
    "\n",
    "6. **Quantum Chemistry**: Eigen decomposition is applied to solve the Schrödinger equation for molecular systems, allowing for the calculation of molecular properties and electronic structure. Applications: Drug discovery, materials science, understanding chemical reactions.\n",
    "\n",
    "7. **Image and Signal Processing**: Eigen decomposition is used for filtering, compression, and feature extraction in image and signal processing. Applications: Image denoising, image compression, speech processing, and feature extraction in computer vision.\n",
    "\n",
    "8. **Control Theory**: In control systems, eigen decomposition helps analyze the stability and response of linear time-invariant systems. Applications: Aerospace engineering (e.g., aircraft control systems), automotive control systems, robotics.\n",
    "\n",
    "9. **Machine Learning**: Eigen decomposition is a fundamental component of techniques like Singular Value Decomposition (SVD), which is used for matrix factorization in recommendation systems, text analysis, and collaborative filtering. Applications: Latent semantic analysis in natural language processing, image compression, and dimensionality reduction in machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1d86a-6da4-44d7-a58a-15ba00b22133",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bb3c775-61e8-4ce3-8b8e-263f78ee364d",
   "metadata": {},
   "source": [
    "Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?\n",
    "\n",
    "Ans. A square matrix can indeed have more than one set of eigenvectors and eigenvalues. In fact, it is quite common for a matrix to have multiple sets of eigenvectors and eigenvalues. However, each set of eigenvectors corresponds to a distinct set of eigenvalues, and the number of unique eigenvalues determines the maximum number of linearly independent eigenvector sets.\n",
    "\n",
    "Here are some important points to consider:\n",
    "\n",
    "1. **Multiple Eigenvector Sets for the Same Eigenvalue**:\n",
    "   \n",
    "   - It is possible for a single eigenvalue to have multiple linearly independent eigenvectors associated with it. These eigenvectors point in different directions but are stretched or compressed by the same factor (the eigenvalue) when the matrix is applied to them.\n",
    "\n",
    "2. **Distinct Eigenvalues Lead to Unique Eigenvector Sets**:\n",
    "\n",
    "   - Different eigenvalues correspond to distinct sets of eigenvectors. These eigenvector sets are linearly independent and represent different directions in the vector space.\n",
    "\n",
    "3. **Number of Eigenvalues Equals Matrix Rank**:\n",
    "\n",
    "   - The number of unique eigenvalues of a matrix is equal to its rank. Therefore, if a matrix has a full rank (i.e., its rank is equal to the number of rows or columns), it will have as many unique eigenvalues as its size, and each eigenvalue will have a unique set of eigenvectors.\n",
    "\n",
    "4. **Degenerate or Repeated Eigenvalues**:\n",
    "\n",
    "   - Some matrices may have repeated eigenvalues, which are also known as degenerate eigenvalues. In such cases, the number of linearly independent eigenvectors corresponding to a repeated eigenvalue can be less than the number of times the eigenvalue is repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f41780-d5e4-4e81-8a46-657a13d147a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "335435a8-5e8c-4a76-ad20-e71699cf0e43",
   "metadata": {},
   "source": [
    "Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning?\n",
    "Discuss at least three specific applications or techniques that rely on Eigen-Decomposition.\n",
    "\n",
    "Ans. The Eigen-Decomposition approach is highly useful in data analysis and machine learning due to its ability to uncover the underlying structure of data, reduce dimensionality, and extract important features. Here are three specific applications and techniques that rely on Eigen-Decomposition in these fields:\n",
    "\n",
    "1. **Principal Component Analysis (PCA)**: PCA is a dimensionality reduction technique that uses Eigen-Decomposition to identify the principal components of a dataset. PCA constructs a set of orthogonal principal components (eigenvectors) of the data's covariance matrix. These principal components represent directions of maximum variance in the data.\n",
    "\n",
    "2. **Singular Value Decomposition (SVD)**: SVD is a matrix factorization technique used in various machine learning tasks, including recommendation systems, text analysis, and image compression. SVD factorizes a matrix into three matrices: U (left singular vectors), Σ (diagonal matrix of singular values), and Vᵀ (right singular vectors). Eigen-Decomposition is employed on the matrix A = XᵀX, where X is the original data matrix.\n",
    "   \n",
    "3. **Kernel Principal Component Analysis (Kernel PCA)**: Kernel PCA extends PCA to non-linear data by using a kernel trick to map data into a higher-dimensional space where PCA is performed. Kernel PCA applies PCA to a transformed feature space, obtained through a kernel function (e.g., radial basis function, polynomial kernel). Eigen-Decomposition is then applied to the kernel matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59bf8f-c2b3-4070-b975-f1c4824f4d66",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
